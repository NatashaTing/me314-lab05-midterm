---
title: "Midterm Assignemnt, ME314 2019"
author: "TING, Zet Ping"
output: 
    html_document: 
    df_print: paged
---

![](images/lse-logo.jpg)

#### Summer School 2019 midsession examination  

# ME314 Introduction to Data Science and Machine Learning 

## Suitable for all candidates


### Instructions to candidates  

* Complete the assignment by adding your answers directly to the RMarkdown document, knitting the document, and submitting the HTML file to Moodle.   
* Time allowed: due 19:00 on Wednesday, 7th August 2019.  
* Submit the assignment via [Moodle](https://shortcourses.lse.ac.uk/course/view.php?id=158).

## Question 1:

This question should be answered using the `Carseats` data set, which is part of the **ISLR** package. This data contains simulated data set containing sales of child car seats at 400 different stores.

```{r importstatements, results='hide', warning=FALSE}

# load required packages
PKGs <- c("class","ISLR","dplyr","ggplot2", "rowr", "caret")

suppressPackageStartupMessages(
  lapply(
    PKGs, library, warn.conflicts = FALSE, character.only = TRUE ,quietly = T
    )
  )

```

***
##### 1.  Fit a regression model predicting Sales using Advertising and Price as predictors.  Interpret the coefficients, the $R^2$, and the Residual standard error from the regression (by explaining each in a few statements).

```{r q1_a}

data(Carseats)

lm.fit1 <- lm(Sales ~ Advertising + Price, data = Carseats)

summary(lm.fit1)
```

<p style="background-color:#e6ffe6; padding: 3%">
An increase of 1 thousand dollars in local **advertising** budget is associated with `0.12` increase in unit sales of car seats, for a given price of the product. Meanwhile, a one-dollar increase in the **price** of car seats is associated with a drop in sales by `0.05` unit, holding advertising budget constant. Together, this model explains `28.2%` of the variation in sales. The total distance between estimated and real values (sum of squared residual, **RSS**), has been minimised at `2.399`. 
</p>
  
    
***  
  
  
##### 2.  Fit a second model by adding Urban as an interactive variable with Advertising.  Interpret the two new coefficients produced by adding this interaction to the Advertising variable that was already present from the first question, in a few statements.

```{r q1_b}

lm.fit2 <- lm(Sales ~ Advertising * Urban + Price, data = Carseats)

summary(lm.fit2)

```

<ul style="background-color:#e6ffe6; padding: 3%">
  <li>For **urban stores**, an additional thousand dollar increase in advertising budget is associated with 0.128015 + -0.006666 = `0.121349`, **0.12 decrease in unit sales**. </li>
  <li>For **non-urban stores**, an additional thousand dollar increase in advertising budget is associated with 0.128015, a **0.13 rise in unit sales**.</li>
</ul>


  
    
***    

##### 3.  Which of these two models is preferable, and why?  

```{r q1_c}
anova(lm.fit1, lm.fit2)
```

<ul style="background-color:#e6ffe6; padding: 3%">
  <li>Between these two models, the one without interaction term is preferred. </li>
  
  <li>The **second model** (with interaction term) **is less preferred** because: </li>  
    <ol>
      <li>the interaction term and new main effect `Urban` term both have **high p-values** and are **not statistically significant**; </li>
      <ul><li>we **fail to reject the null hypothesis in ANOVA** where  
    $$H_0: \widehat{\beta}_\text{Urban} == \widehat{\beta}_\text{Urban*Advertising} == 0$$</li></ul>
      <li>there is **no noticable increase in model explanatory power** from including interaction term, with $R^2$ constant at `0.2819`when rounded;  </li>
      <li>**adjusted-$R^2$ is lower** because of penalty from including 2 more variables.  </li>
      
    </ol>
</ul>
  
  
  
## Question 2:

You will need to load the core library for the course textbook and any other libraries you find suitable to answer the question: 


This question should be answered using the `Weekly` data set, which is part of the **ISLR** package. This data contains 1,089 weekly stock returns for 21 years, from the beginning of 1990 to the end of 2010.

#### 1.   Perform exploratory data analysis of the `Weekly` data (produce some numerical and graphical summaries). Discuss any patterns that emerge. 

```{r q2_a_explore}
data(Weekly)

attach(Weekly)

sum(is.na(Weekly))      # = 0

summary(dplyr::select_if(Weekly, is.numeric))   

# check how many weeks are included in each year
count_by_years <- group_by(Weekly, Year) %>% 
                                    summarise(n = n())

count_by_years


# ---- explore Lagged returns
# generate a correlation matrix between the lag variables 
corr.mat <- dplyr::select(Weekly, -Direction) %>%
          dplyr::mutate(as.numeric(Direction)) %>%
                      cor() %>% 
                      round(3) 
corr.mat

```

```{r}
# plot the correlation between the lag variables
plot(corr.mat[which(rownames(corr.mat)=="Lag2"), -which(rownames(corr.mat)=="Lag2")], 
              type="b", col=2,xlab="index (other variable)", ylab="correlation")
lines(corr.mat[which(rownames(corr.mat)=="Lag1"),-which(rownames(corr.mat)=="Lag1")],
              type="b", col=3, ylab="")
lines(corr.mat[which(rownames(corr.mat)=="Lag3"),-which(rownames(corr.mat)=="Lag3")], 
              type="b", col=4, ylab="")
lines(corr.mat[which(rownames(corr.mat)=="Lag4"),-which(rownames(corr.mat)=="Lag4")], 
              type="b", col=5, ylab="")
lines(corr.mat[which(rownames(corr.mat)=="Lag5"),-which(rownames(corr.mat)=="Lag5")], 
              type="b", col=6)
legend("bottomright", legend = c("Lag2", "Lag1", "Lag3", "Lag4", "Lag5"), 
       col = 2:6, lwd = 2)

```

```{r}

# ---- explore Direction

plot(Today, Direction)
abline( v = 0 , col = "red")

ggplot(data = Weekly)  + 
  geom_point(mapping = aes(x = Year, y = Volume)) + 
  geom_smooth(mapping = aes(x = Year, y = Volume), col="red")


print(mean(as.numeric(Direction)))

```

<ul style="background-color:#e6ffe6; padding: 3%">
  ##### Observation 
  <li> Data is dated from 1990 to 2010.</li>  
  
  <li> **Returns**
    <ul>
      <li>From the correlation matrix plot, a lot of the **lag variables are correlated** with percentage returns from 2 weeks (periods) before them. </li>
      <li>However, the autocorrelation pattern is erratic. There is no other clear trend.</li>
    </ul>
  </li>
  
  <li> **`Volume` **
    <ul> 
    <li>`Volume` traded per day is **highly variant** week-to-week. Its standard deviation is `(sd(Volume) / mean(Volume)) %>% round(2)` = `1.07` times its mean of `1.57`.  </li>
    <li>This means that on average, for any given week we can expect the next week's volume to be 1.68 billion shares more or less than the previous week. </li>
    <li>Volume also exhibits an **increasing trend over time**. </li>
    </ul>
  </li>
  
  <li>**`Direction`**
    <ul>
      <li>Direction seems to be evaluated relative to the percentage for this week. Therefore, `Direction` will cause a **perfect separation** in regression. </li>
      <li>The only factor variable in the dataset `Direction`, has two levels. Because investors may have a preference for the "Up" movement, a logistic regression of it may have **class imbalance** concerns. </li>
      <li>There is **more bullish days** than bearish days in this dataset. </li>
    </ul>
  </li>
</ul>

  
    
    
#### 2. Fit a logistic regression with `Direction` as the response and different combinations of lag variables plus `Volume` as predictors. Use the period from 1990 to 2008 as your training set and 2009-2010 as your test set. Produce a summary of results. 

    Do any of the predictors appear to be statistically significant in your training set? If so, which ones?
    

```{r}

Weekly.test <- Weekly %>% dplyr::filter(Year %in% c(2009,2010))
Weekly.train <- Weekly %>% dplyr::filter(!Year %in% c(2009, 2010))

# test if sensible after splitting
isTRUE(nrow(Weekly.train) + nrow(Weekly.test) == nrow(Weekly))
isTRUE(ncol(Weekly.train) == ncol(Weekly.test))

logit.fit_all <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume , 
                 family = "binomial", 
                 data = Weekly.train)

logit.fit_notf4 <- glm(Direction ~ Year + Lag1 + Lag2 + Lag3 + Lag4 + Volume,
                      family = "binomial",
                      data = Weekly.train)

logit.fit_notf3 <- glm(Direction ~ Year + Lag1 + Lag2 + Lag3 + Volume,
                      family = "binomial",
                      data = Weekly.train)

logit.fit_notf2 <- glm(Direction ~ Year + Lag1 + Lag2 + Volume,
                      family = "binomial",
                      data = Weekly.train)

logit.fit_notf1 <- glm(Direction ~ Year + Lag1 + Volume,
                      family = "binomial",
                      data = Weekly.train)

logit.fit_nolag <- glm(Direction ~ Year + Volume,
                      family = "binomial",
                      data = Weekly.train)

allfit <- c(logit.fit_all, logit.fit_notf4, logit.fit_notf3, logit.fit_notf2, logit.fit_notf1, logit.fit_nolag)

# get list of p-values
logit.fit_allp <- summary(logit.fit_all)$coefficients[,4]
logit.fit_notf4p <- summary(logit.fit_notf4)$coefficients[,4]
logit.fit_notf3p <- summary(logit.fit_notf3)$coefficients[,4]
logit.fit_notf2p <- summary(logit.fit_notf2)$coefficients[,4]
logit.fit_notf1p <- summary(logit.fit_notf1)$coefficients[,4]
logit.fit_nolagp <- summary(logit.fit_nolag)$coefficients[,4]

# this is a table of p-values
tryll <- cbind.fill(logit.fit_allp, logit.fit_notf4p, logit.fit_notf3p, 
                    logit.fit_notf2p,logit.fit_notf1p, logit.fit_nolagp, fill = NA)

# intercept aside, it looks like only Lag1 is slightly signif. (p value 0.03)
tryll

# verify that table is correcly representing p-values from logistic fit
summary(logit.fit_all)
logit.fit_allp[2]         

```

<p style="background-color:#e6ffe6; padding: 3%">
Only `Lag1` is moderately significant with a p-value of 0.03, when `Direction` is regressed on all other lag variables and trade `Volume` on a logistic regression.</p>

  
    
      
      
#### 3.  From your test set, compute the confusion matrix, and calculate accuracy, precision, recall and F1. 
     
    Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression, and what can you learn from additional measures of fit like accuracy, precision, recall, and F1.

```{r}

set.seed(123)

logit.fit_all_model <- train(
  Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
  data = Weekly.train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10))

pred_class <- predict(logit.fit_all_model, Weekly.test)

confusionMatrix(
  mode = "everything",
  data = relevel(pred_class, ref = "Down"), 
  reference = relevel(Weekly.test$Direction, ref = "Down")
)

```

<p style="background-color:#e6ffe6; padding: 3%">
The confusion matrix shows that the model **predicted correctly 554 times** in total. The model made **431 incorrect predictions** in total: in **361** occasions having predicted "Up" when market is down; and **70** times predicted "Down" when market goes up.</p>

<ol style="background-color:#e6ffe6; padding: 3%">
  <li> _Note_: "bearish" means market has a downward movement. "bullish" means market has an upward movement. </li>
  <li>**Accuracy** = `0.4615` 
    <ul>
      <li>`48` Correct gueses / `104` Total Predictions</li>
      <li> **Model is correct in its prediction 56% of the time.** </li>
    </ul>
  </li>
  
  <li>**Sensitivity** (a.k.a. **Recall**, True Positive Rate ) = `0.7209`
    <ul>
      <li> `1` - False Positive = `1` - (`44` / `44 + 17`) = `1` - `0.2791` = `0.7209`</li>
      <li> `31` True Positive / `43` Total True Positive = `0.7209` </li>
      <li> **The model correctly predicted 72% of the number of total actual bearish weeks.** While this is not bad, it means that **28% of the time, it wrongly predicted that the market would go up when it actually went down** Because of class imbalance this is likely drastic.  </li>
    </ul>
  </li>
  
  <li>**Specificity** (a.k.a. true negative rate) = `0.2787`
    <ul>
      <li>`17` True Negative / `61` Total True Negative = `0.2787`</li>
      <li>The model only **correctly predicted 28% of the number of total actual bullish week**s.  This means that 72% of the times, it wrongly predicted that the market would go down when it actually went up. </li>
    </ul>
  </li>
  
  <li>**Precision** = `0.4133` 
    <ul>
      <li>Def: `31` True Positive / (`31` True Positive + `44` False Positive)] </li>
      <li> `31` / `75` = 0.4133 </li>
      <li>**The model correctly predicted 41% of the number of total predicted bearish weeks.**</li>
    </ul>
  </li>
  
  <li>**F1** = `0.5254 ` 
    <ul>
      <li>F1 = 2 * [(Precision * Recall) / (Precision + Recall)] </li>
      <li>2 * [(0.4133 * 0.7209) / (0.4133 + 0.7209)]</li>
    </ul>
  </li>
  
</ol>

  
  
#### 4.  (Extra credit) Experiment with alternative classification methods. 
  Present the results of your experiments reporting method, associated confusion matrix, and measures of fit on the test set like accuracy, precision, recall, and F1.
    
    
```{r import_for_tree}

# Modeling packages
library(rpart)       # direct engine for decision tree application
library(rpart.plot)  # for plotting decision trees
library(vip)         # for feature importance
library(pdp)         # for feature effects

```

```{r Q2-decisiontree}

set.seed(123)

weekly_dt1 <- rpart(
  formula = Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
  data    = Weekly.train,
  method = "class"
) 

rpart.plot(weekly_dt1)

plotcp(weekly_dt1)

pre_class_dt1 <- predict(weekly_dt1, Weekly.test, type="class")

confusionMatrix(
  mode = "everything",
  data = relevel(pre_class_dt1, ref = "Down"), 
  reference = relevel(Weekly.test$Direction, ref = "Down")
)

```
  
<p style="background-color:#e6ffe6; padding: 3%">
As the tree diagram shows, a decision tree model of 7 levels is fitted on the training data set. The Complexity Parameter Table shows that `0.022` looks like a good choice of cp for pruning, as it is the least expensive value for which the mean lies below the horizontal line of relative error. However, this tree has not been pruned.
</p>

```{r Q2-bagging}

library(randomForest)
library(ipred)       # for fitting bagged decision trees

set.seed(123)

# train bagged model
week_bag1 <- bagging(
  formula = Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
  data = Weekly.train,
  nbagg = 100,  
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)

# based on training data, Out-of-bag estimate of misclassification error:  0.467 

# try doing conf matrix using manual means
weekly_bag_probs <-  predict(week_bag1, newdata=Weekly.test,
                      type = "class")
table(weekly_bag_probs, Weekly.test$Direction)
mean(weekly_bag_probs != Weekly.test$Direction)     
# this means no info error rate is 1 - 0.5096154 = 0.4903846

#validate with package-produced matrix
confusionMatrix(
  mode = "everything",
  data = relevel(weekly_bag_probs, ref = "Down"), 
  reference = relevel(Weekly.test$Direction, ref = "Down")
)

```

<p style="background-color:#e6ffe6; padding: 3%">
**Bagging**: 
This model applies 100 bootstrap replications on the sample data from test set. The accuracy rate is 0.4904, leaving **an error rate of 0.5096**. which is higher than out-of-bag (OOB) estimate of misclassification error 0.467, but lower than no-information error rate of 0.5865.
</p>
    

```{r randomForest}

set.seed(123)

rf.weekly <-  randomForest(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
                           data=Weekly.train, 
                           mtry=2)

weekly_rf_probs <-  predict(rf.weekly, newdata=Weekly.test, type = "class")

confusionMatrix(
  mode = "everything",
  data = relevel(weekly_rf_probs, ref = "Down"), 
  reference = relevel(Weekly.test$Direction, ref = "Down")
)


```
<p style="background-color:#e6ffe6; padding: 3%">
**Random Forest**: 
A random forest model is applied on the training data. This tree chooses 2 random variables as candidates to optimise for at each split. This model has an accuracy and F1 of 0.5, leaving an error rate of 0.5, which are both lower than no-information error rate of 0.5865.
</p>

*** 

<p style="background-color:#e6ffe6; padding: 3%">
**Summary of results**
</p>
***

Model         | Sensitivity    | F1            | No Info Error Rate  |
------------- | -------------  | ------------- | -------------       |
Logistic reg  | 0.7209         | 0.5254        | 0.5865              |
Decision Tree | 0.6279         | 0.5347        | 0.5865              |
Bagging       | 0.6279         | 0.5047        | 0.5865              |
Random Forest | 0.6047         | 0.5000        | 0.5865              |

*** 
<p style="background-color:#e6ffe6; padding: 3%">
Among the 4 methods, logistic regression has the highest proportion of the positive class correctly predicted (a.k.a. sensitivity). However, the decision tree method has the highest harmonic average of the precision and recall F1 at 0.5347. 
</p>
